---
title: 'Predictive Analysis Project2 :'
author: Jered Ataky, Matthew Baker, Christopher Bloome, David Blumenstiel, Dhairav Chhatbar
date: "7/7/2021"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(dplyr)
library(readxl)
library(skimr)
library(tidyr)
library(kableExtra)
library(mice)
library(VIM)
library(corrplot)
library(ggcorrplot)
library("caret")
library("glmnet")
library("earth")
library("pls")
library("e1071")
library("xgboost")
```


## Introduction
<insert intro text>

## Data Prepration
<insert data load text>
```{r, message=FALSE, warning=FALSE, message=FALSE}

train_dataset <- read.csv("https://raw.githubusercontent.com/jnataky/Predictive_Analytics/main/Project2/StudentData%20-%20TO%20MODEL.csv")
evaluation_dataset <- read.csv("https://raw.githubusercontent.com/jnataky/Predictive_Analytics/main/Project2/StudentEvaluation-%20TO%20PREDICT.csv")


evaluation_dataset$PH <- NULL  #Having an empty column get's in the way for most of this


head(train_dataset)



```

## Explorative Data Analysis
<description/discussion of each visual>
```{r}
skim(train_dataset)
```

```{r, warning=FALSE}
aggr(train_dataset, col=c('#F8766D','#00BFC4'), numbers=TRUE, sortVars=TRUE, labels=names(train_dataset), cex.axis=.7, gap=3, ylab=c("Missing data","Pattern"))
```

```{r, warning=FALSE}
train_dataset %>% select(PH) %>% ggplot( aes(PH)) + geom_density(fill="steelblue", bins = 30)
```


```{r, fig.height=10, warning=FALSE, message=FALSE}

train_dataset %>% select(-PH, -Brand.Code) %>% 
  tidyr::gather(key = "variable", value = "measurement",everything()) %>% 
  arrange(desc(variable)) %>%
  ggplot(aes(measurement)) + geom_density(position=position_dodge(), fill="steelblue") +
  facet_wrap(~variable, scales = "free") +
  theme_minimal() + 
  ggtitle("Distribution of Predictor Variables")
```

```{r, fig.height=10}
q <- cor(train_dataset%>%select(-Brand.Code), use = "na.or.complete")
ggcorrplot(q, type = "lower", outline.color = "white", hc.order = TRUE,
           colors = c("#6D9EC1", "white", "#E46726"),
           lab = TRUE, show.legend = FALSE, tl.cex = 8, lab_size = 3) 



```



## Data Transformation


Below will impute missing data via pmm, and rename the first column and change it to factor

```{r}
preprocess <- function(df) {
  colnames(df)[1] <- "Brand"  #Changes name of first column to somthing less obtuse
  df$Brand <- as.factor(df$Brand)#Changes brand to factor
  
  #Uses MICE for imputation of missing values.  Going with mostly defaults.
  imputed <- mice(df,
     m = 5,
     maxit = 5,
     seed = 10,
     trace = FALSE)
  
  df <- complete(imputed)
  
  
  return(df)
}

train_dataset <- preprocess(train_dataset)
evaluation_dataset <- preprocess(evaluation_dataset)
```


## Modeling

Going to try to use caret for implementing most of these; it will make things alot easier.

First, we need split off a test set to judge performance.  We'll do 80:20 train/test

```{r}
set.seed(1234567890)
splitdex <- createDataPartition(train_dataset$PH, p = 0.8, list = FALSE)


train <- train_dataset[splitdex,]
test <- train_dataset[-splitdex,]

```






### LASSO

We can probably do away with alot of these values and get a simpler model.  LASSO, a penalized model, will aim to do such  We'll implemet this with caret for simplicity.

```{r}
set.seed(1234567890) #So you see the same thing I do

#got help from: http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/
#and: https://stackoverflow.com/questions/57712116/regarding-preprocessing-in-lasso-using-caret-package-in-r

grid <- expand.grid(alpha = 1,
                    lambda =10^seq(-5, 5, length = 1000)) #Lot's of lambda values to choose from

tc = trainControl(method = "cv",   #Cross-validation
                  number = 10)


lassoFit <- caret::train(PH ~ ., data = train, 
                  method = "glmnet",   #glmnet lets us fit penalized maximum likelihood glms (lasso, ridge, elastic) 
                  preProcess = c("center", "scale"), #Data needsa to be centered and scaled for this 
                  tuneGrid = grid,
                  trControl = tc)

lassoFit$bestTune  

coef(lassoFit$finalModel,lassoFit$bestTune$lambda)

varImp(lassoFit$finalModel, lambda = lassoFit$bestTune$lambda)


```

The best model was made using a lambda of 0.00011 ish.  This value was the most effective in terms of reducing RMSE, but it's so low it didn't really get rid of any coefficients .  For our purposes, an accurate model is more important than a simple one, so we'll keep the low alpha.  Let's see how it does on the validation set.

```{r}
lasso_predictions <- predict(lassoFit, test)


print(paste("RMSE: ", RMSE(lasso_predictions, test$PH), "    R2: ", caret::R2(lasso_predictions, test$PH)))


```

Performance wise, looks fairly good.  Let's compare it to some other models.


### Elastic-Net

Easy enough to try out one of these, having alerady made the LASSO model.  For this, we just need to give some values of alpha to choose from.

```{r}
set.seed(1234567890) #So you see the same thing I do

#got help from: http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/
#and: https://stackoverflow.com/questions/57712116/regarding-preprocessing-in-lasso-using-caret-package-in-r



tc = trainControl(method = "cv",   #Cross-validation
                  number = 10)


elasticFit <- caret::train(PH ~ ., data = train, 
                  method = "glmnet",   #glmnet lets us fit penalized maximum likelihood glms (lasso, ridge, elastic) 
                  preProcess = c("center", "scale"), #Data needsa to be centered and scaled for this 
                  tuneLength = 20, #Going to let it choose alpha and lambda without any sugestions
                  trControl = tc)

elasticFit$bestTune  

coef(elasticFit$finalModel,elasticFit$bestTune$lambda)

varImp(elasticFit$finalModel, lambda = elasticFit$bestTune$lambda)

```

It chose a similarly small lambda as the LASSO model.  Let's see how it performs on the test set.

```{r}
elastic_predictions <- predict(elasticFit, test)


print(paste("RMSE: ", RMSE(elastic_predictions, test$PH), "    R2: ", caret::R2(elastic_predictions, test$PH)))


```

Does only slightly better than LASSO.


### MARS

Let's try one of these.  I'm a bit concerned that the collinear variables could lead to overfitting, but we can test things out on the validation set.


```{r}
set.seed(1234567890)

#got help from here: https://bradleyboehmke.github.io/HOML/mars.html

tc = trainControl(method = "cv",   #Cross-validation
                  number = 10)

grid <- expand.grid(degree = 4:7,   #automatic tune won't let you try more than one of these
                    nprune = seq(60, 150, length = 10))

marsFit <- caret::train(PH ~ ., data = train, 
                        method = "earth",   #earth has mars model.  I know how that sounds 
                        preProcess = c("center", "scale"), #why not
                        tuneGrid = grid, 
                        trControl = tc)



marsFit$bestTune

varImp(marsFit)
```


An initital run showed better performance with the highest degree and number of number of tunes used.  Above, the model was re-trained using higher degree and term numbers (not including the origional set to speed this up).

It found 60 terms and 5 degrees to have the best performance., with an RMSE and R2 around 0.12 and 0.52 respectively.  Let's evaluate on the validation set.

```{r}
mars_predictions <- predict(marsFit, test)


print(paste("RMSE: ", RMSE(mars_predictions, test$PH), "    R2: ", caret::R2(mars_predictions, test$PH)))


```

It seems the concerns over overfitting were valid.  The RMSE and R2 are not nearly as good when tested on the validation set.  


### Partial Least Squares

This should handle multicolinearity better, and should do well considering the number of variables.

```{r}
set.seed(1234567890) #so you see what I do

#took help from here: http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/152-principal-component-and-partial-least-squares-regression-essentials/

tc = trainControl(method = "cv",   #Cross-validation
                  number = 10)

plsFit <- caret::train(PH ~ ., data = train, 
                        method = "pls",   # from the "pls" package
                        preProcess = c("center", "scale"), 
                        tuneLength = 32,  #n_var - 1
                        trControl = tc)

plsFit
```

Let's try it out:

```{r}
pls_predictions <- predict(plsFit, test)


print(paste("RMSE: ", RMSE(pls_predictions, test$PH), "    R2: ", caret::R2(pls_predictions, test$PH)))


```

Does just about the same.

### SVM (for regression)

Let's try a SVM for regression (not classification)

```{r}
set.seed(1234567890) #so you see what I do

#took help from here: https://stackoverflow.com/questions/49543307/svm-with-radial-kernel-for-numeric-response-in-caret-package

tc = trainControl(method = "cv",   #Cross-validation
                  number = 5)

svmFit <- caret::train(PH ~ ., data = train, 
                        method = "svmRadial",   # from the "e1071" package
                        preProcess = c("center", "scale"), 
                        tuneLength = 10, 
                        trControl = tc)

svmFit
```

Let's see how it does on the validation set.

```{r}
svm_predictions <- predict(svmFit, test)


print(paste("RMSE: ", RMSE(svm_predictions, test$PH), "    R2: ", caret::R2(svm_predictions, test$PH)))


```

It does particularly well.  


### Random Forest

```{r}
set.seed(1234567890) #so you see what I do

#took help from here: 

tc <- trainControl(method = "cv",   #Cross-validation
                  number = 5)


grid <- expand.grid(.mtry = c(18)) 

rfFit <- caret::train(PH ~ ., data = train, 
                      method = "rf",   
                      preProcess = c("center", "scale"), 
                      tuneGrid = grid,  
                      trControl = tc,
                      ntrees = 1000)

rfFit
```

Promising.  I tried several combinations of mtrys and ntrees, and above did just as well in a shorter time.

```{r}
rf_predictions <- predict(rfFit, test)


print(paste("RMSE: ", RMSE(rf_predictions, test$PH), "    R2: ", caret::R2(rf_predictions, test$PH)))


```

The best model so far.  better on the testing set than the training set


### Neural Network

Not particularly transparent, but might get the job done.

```{r}
set.seed(1234567890) #so you see what I do

#took help from here: 

tc <- trainControl(method = "cv",   #Cross-validation
                  number = 5)

grid <- expand.grid(decay = 10^seq(-3,-0.5,length = 10),
                    size = 1:5)

nnFit <- caret::train(PH ~ ., data = train, 
                      method = "nnet",   
                      preProcess = c("center", "scale"), 
                      linout=TRUE, 
                      trace = FALSE,
                      tuneGrid = grid,
                      trControl = tc)

nnFit
```

Selected some values right in the middle, but ultimately doesn't look as promising as random forest or svm.

```{r}
nnet_predictions <- predict(nnFit, test)


print(paste("RMSE: ", RMSE(nnet_predictions, test$PH), "    R2: ", caret::R2(nnet_predictions, test$PH)))


```

Decent, but not the best so far.




### Cubist (M5)

A strange take on decision trees.  Random forest did well; maybe this will build on that?

```{r}
set.seed(1234567890) #so you see what I do

#took help from here: 

tc <- trainControl(method = "cv",   #Cross-validation
                  number = 5)

grid = expand.grid(committees = 100,
                   neighbors = 8)

cubistFit <- caret::train(PH ~ ., data = train, 
                      method = "cubist",   
                      preProcess = c("center", "scale"),
                      tuneGrid = grid,
                      trControl = tc)

cubistFit
```

I trained this with additional parameter combinations to those tried above (not included incase you want to run this in under half an hr).  In the above attempt, the best combination was found

```{r}
cubist_predictions <- predict(cubistFit, test)


print(paste("RMSE: ", RMSE(cubist_predictions, test$PH), "    R2: ", caret::R2(cubist_predictions, test$PH)))


```

Just about on par with the random forest


### eXtreme Gradient Boosting

Saw on list, Sounded cool.  Gradient boosted decision trees.

```{r}
set.seed(1234567890) #so you see what I do

#took help from here: 

tc <- trainControl(method = "cv",   #Cross-validation
                  number = 5)

grid <- expand.grid(lambda = c(1.39 * 10^-5, 10^-5, 1.90 * 10^-5),
                    alpha = c(0.0001,0.00046, 0.001),
                    nrounds = c(80,90,100),
                    eta = 0.3)

xgbFit <- caret::train(PH ~ ., data = train, 
                      method = "xgbLinear",   
                      preProcess = c("center", "scale"),
                      tuneGrid = grid,
                      trControl = tc)

xgbFit
```

parameters were tuned individually.  The last iteration of training is shown above.  This model seems to perform well, but lets double check on the test set.

```{r}
xgb_predictions <- predict(xgbFit, test)


print(paste("RMSE: ", RMSE(xgb_predictions, test$PH), "    R2: ", caret::R2(xgb_predictions, test$PH)))


```

It does very well.


### K-Nearest Neighbors

Another basic but good one.

```{r}
set.seed(1234567890) #so you see what I do

#took help from here: 

tc <- trainControl(method = "cv",   #Cross-validation
                  number = 5)

grid <- expand.grid(k = 1:10)

knnFit <- caret::train(PH ~ ., data = train, 
                      method = "knn",   
                      preProcess = c("center", "scale"),
                      tuneGrid = grid,
                      trControl = tc)

knnFit
```

```{r}
knn_predictions <- predict(knnFit, test)


print(paste("RMSE: ", RMSE(knn_predictions, test$PH), "    R2: ", caret::R2(knn_predictions, test$PH)))


```


Not bad, but not the best.





## Best Model Analysis

### Important variables

```{r}
varImp(rfFit)
```



## Forcasting & Conclusion

We'll go with random forest.  It performed among the best on the test set.

```{r}
evaluation_dataset$PH <- predict(rfFit, evaluation_dataset)


```


